<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>wyq&#39;s blog</title>
  
  <subtitle>纸上得来终觉浅，绝知此事要躬行</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://hf136.github.io/"/>
  <updated>2019-01-17T15:16:33.588Z</updated>
  <id>http://hf136.github.io/</id>
  
  <author>
    <name>wyq</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>循环神经网络（Recurrent Neural Network）</title>
    <link href="http://hf136.github.io/2018/12/28/rnn/"/>
    <id>http://hf136.github.io/2018/12/28/rnn/</id>
    <published>2018-12-28T13:40:59.000Z</published>
    <updated>2019-01-17T15:16:33.588Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在自然语言处理（NLP）中，需要处理的数据通常都是不定长的。例如，我们要构建一个神经网络模型，将下面这两句话翻译成英文：</p><ul><li>这一世诺言从不曾忘。</li><li>深度学习的概念源于人工神经网络的研究。</li></ul><p>这两句话的长度是不一样的，一般的神经网络输入的特征纬度是固定的，显然不能很好的解决这个问题，于是便出现了循环神经网络（Recurrent Neural Network，RNN）。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="基本的循环神经网络"><a href="#基本的循环神经网络" class="headerlink" title="基本的循环神经网络"></a>基本的循环神经网络</h3><p>一个最基本的循环神经网络由输入层，隐藏层和输出层构成，如下图所示：</p><p><img src="/resource/images/rnn-1.jpg" alt="rnn-1"></p><a id="more"></a><p>这里的x、s、o分别是输入层、隐藏层和输出层，它们都是一个向量；W、U、V是连接层与层之间的权重矩阵。<br>RNN和一般的神经网络最大不同在于：</p><blockquote><p>RNN多了一个从隐藏层到隐藏层($s =&gt; s$)的过程，使RNN拥有了“记忆”的功能。<br>(注意：这里的s要把他它看层多个隐藏层的多个神经单元，s是隐藏层单元构成的向量)</p></blockquote><p>在RNN网络中，我们需要引入一个时间（顺序）的概念，我们把上图展开，RNN可以画成这样：</p><p><img src="/resource/images/rnn-2.jpg" alt="rnn-2"></p><p>从图中可以看到，t 时刻的RNN网络输入值是 $x_t$，输出值是 $o_t$，隐藏层的值是 $s_t$，它的值取决于输入值 $x_t$ 和 t-1 时刻的隐藏层的值 $s_{t-1}$。</p><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><p>RNN的每时间步的计算过程如下：<br>$$<br>\begin{aligned}<br>s_t =&amp; g(Ux_t + Ws_{t-1} + b) \\<br>o_t =&amp; f(Vs_t)<br>\end{aligned}<br>$$</p><p>其中，g、f 是激活函数，s 为隐藏层的值， b 是偏差项；隐藏层 s 的初始值 $s_0$ 为零向量。</p><p>可以看出，RNN网络最后输出的结果受到所有输入序列 $x_1, x_2 … x_T$ 的影响。因为隐藏 $s_{t-1}$ 保存了前面 t-1 个 x 值的结果，隐藏层 s 充当了一个“记忆”的角色。</p><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>同样是求使得损失函数最小的权重 U 、 W 、V 、b；损失函数的形式根据具体的任务会有所不同。</p><h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>这里RNN使用到的计算梯度的算法是BPTT（Back Propagation Trough Time），加上了时间的概念，是一种基于时间的反向传播算法。</p><p>虽然名字听上去很高大上的样子，但其实并不复杂，和普通的反向传播算法也差不多，把RNN展开之后，一样可以使用链式求导，这其实就很简单了。</p><p><img src="/resource/images/rnn-3.png" alt="RNN backward"></p><p>如上图所示，把 RNN 展开之后，RNN 每一时刻的反向传播求导过程和普通的神经网络是一样。根据任务的不同，有可能每一个时刻都有误差传递，也可能只有最后一个时刻有误差传递。</p><p>展开后的 RNN 可以看成共享权重的全连接神经网络模型，只要使用链式求导分别求出每一个时刻的权重梯度，最后再把所有时刻的梯度相加求和就可以得到最终的 RNN 权重梯度。</p><h2 id="梯度爆炸和梯度消失"><a href="#梯度爆炸和梯度消失" class="headerlink" title="梯度爆炸和梯度消失"></a>梯度爆炸和梯度消失</h2><p>在序列很长的时候，RNN 模型训练过程中，很容易出现梯度爆炸（梯度很大）或者梯度消失（梯度几乎为0）的问题，导致模型无法正常拟合。</p><p>这是为什么呢？</p><p>链式求导求解梯度的过程其实一个连乘的过程：<br>$$<br>\frac{\partial{S_n}}{\partial{S_{n-1}}} \frac{\partial{S_{n-1}}}{\partial{S_{n-2}}} … \frac{\partial{S_2}}{\partial{S_1}}<br>$$<br>当序列很长的时候，如果每个阶段梯度都大于1的话，梯度就会爆炸，比如: $10^9$ ；如果每个阶段梯度都小于1的话，梯度就会消失，比如: $0.1^9$ ；</p><p>对于梯度消失，其实指的是长距离的梯度消失，相隔距离过长的输入值 $x_t$ 相互之间影响很小。由于整个模型的梯度是各个时刻梯度之和，所以模型的梯度还不会消失。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>这里实现了一个简单的 RNN 模型，其中激活函数使用的是 Relu 激活函数。<a href="https://github.com/hf136/models/tree/master/RNN" target="_blank" rel="noopener">完整代码</a></p><p>一个 RNN 时间步的计算过程，其实就和普通的神经网络是一致的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNCell</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    一个 RNN 时间步的计算过程</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_size, hidden_size)</span>:</span></span><br><span class="line">        self.in_size = in_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.w_i2h = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, (in_size, hidden_size))</span><br><span class="line">        self.w_h2h = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, (hidden_size, hidden_size))</span><br><span class="line">        self.bias = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, (<span class="number">1</span>, hidden_size))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x[x &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, h)</span>:</span></span><br><span class="line">        self.i2h = x.dot(self.w_i2h)</span><br><span class="line">        self.h2h = h.dot(self.w_h2h)</span><br><span class="line">        self.h_relu = self.relu(self.i2h + self.h2h + self.bias)</span><br><span class="line">        <span class="keyword">return</span> self.h_relu</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad, i, h)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> i.ndim == <span class="number">1</span>:</span><br><span class="line">            i = np.expand_dims(i, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> h.ndim == <span class="number">1</span>:</span><br><span class="line">            h = np.expand_dims(h, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self.grad_h_relu = grad</span><br><span class="line">        self.grad_h = self.grad_h_relu.copy()</span><br><span class="line">        self.grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        self.grad_w_h2h = h.T.dot(self.grad_h)</span><br><span class="line">        self.grad_w_i2h = i.T.dot(self.grad_h)</span><br><span class="line">        self.grad_bias = self.grad_h</span><br><span class="line">        self.grad_h_in = self.grad_h.dot(self.w_h2h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.grad_h_in</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weight</span><span class="params">(self, lr)</span>:</span></span><br><span class="line">        self.w_i2h -= lr * self.grad_w_i2h</span><br><span class="line">        self.w_h2h -= lr * self.grad_w_h2h</span><br><span class="line">        self.bias -= lr * self.grad_bias</span><br></pre></td></tr></table></figure><p>完整的 RNN 序列计算过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    完整的 RNN 序列计算过程</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_size, hidden_size)</span>:</span></span><br><span class="line">        self.h_state = []</span><br><span class="line">        self.in_size = in_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.rnncell = RNNCell(in_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.h_state = []</span><br><span class="line">        self.x = x</span><br><span class="line">        h = np.zeros(self.hidden_size)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">            self.h_state.append(h)</span><br><span class="line">            h = self.rnncell.forward(i, h)</span><br><span class="line">        self.h_out = h</span><br><span class="line">        <span class="keyword">return</span> self.h_out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad)</span>:</span></span><br><span class="line">        self.grad_w_i2h = np.zeros((self.in_size, self.hidden_size))</span><br><span class="line">        self.grad_w_h2h = np.zeros((self.hidden_size, self.hidden_size))</span><br><span class="line">        self.grad_bias = np.zeros((<span class="number">1</span>, self.hidden_size))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.h_state) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            x = self.x[i]</span><br><span class="line">            h = self.h_state[i]</span><br><span class="line">            grad = self.rnncell.backward(grad, x, h)</span><br><span class="line">            self.grad_w_i2h += self.rnncell.grad_w_i2h</span><br><span class="line">            self.grad_w_h2h += self.rnncell.grad_w_h2h</span><br><span class="line">            self.grad_bias += self.rnncell.grad_bias</span><br><span class="line">        <span class="keyword">return</span> grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weight</span><span class="params">(self, lr)</span>:</span></span><br><span class="line">        self.rnncell.w_i2h -= lr * self.grad_w_i2h</span><br><span class="line">        self.rnncell.w_h2h -= lr * self.grad_w_h2h</span><br><span class="line">        self.rnncell.bias -= lr * self.grad_bias</span><br><span class="line">        <span class="keyword">return</span> self.rnncell.w_i2h, self.rnncell.w_h2h, self.rnncell.bias</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">https://zybuluo.com/hanbingtao/note/541458</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;在自然语言处理（NLP）中，需要处理的数据通常都是不定长的。例如，我们要构建一个神经网络模型，将下面这两句话翻译成英文：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这一世诺言从不曾忘。&lt;/li&gt;
&lt;li&gt;深度学习的概念源于人工神经网络的研究。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两句话的长度是不一样的，一般的神经网络输入的特征纬度是固定的，显然不能很好的解决这个问题，于是便出现了循环神经网络（Recurrent Neural Network，RNN）。&lt;/p&gt;
&lt;h2 id=&quot;模型&quot;&gt;&lt;a href=&quot;#模型&quot; class=&quot;headerlink&quot; title=&quot;模型&quot;&gt;&lt;/a&gt;模型&lt;/h2&gt;&lt;h3 id=&quot;基本的循环神经网络&quot;&gt;&lt;a href=&quot;#基本的循环神经网络&quot; class=&quot;headerlink&quot; title=&quot;基本的循环神经网络&quot;&gt;&lt;/a&gt;基本的循环神经网络&lt;/h3&gt;&lt;p&gt;一个最基本的循环神经网络由输入层，隐藏层和输出层构成，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/images/rnn-1.jpg&quot; alt=&quot;rnn-1&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>站点导航</title>
    <link href="http://hf136.github.io/2018/12/03/navigation/"/>
    <id>http://hf136.github.io/2018/12/03/navigation/</id>
    <published>2018-12-03T15:09:46.000Z</published>
    <updated>2019-01-17T15:21:29.425Z</updated>
    
    <content type="html"><![CDATA[<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ul><li><a href="/2018/09/19/EasyML-1/">机器学习简介</a></li><li><a href="/2018/09/27/linear-regression/">线性回归</a></li><li><a href="/2018/11/03/logistic-regression/">逻辑回归</a></li><li><a href="/2018/11/18/neural-network/">人工神经网络</a></li><li><a href="/2018/12/28/rnn/">循环神经网络</a></li></ul><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul><li>lstm</li><li>cnn</li><li>word2vec</li><li>textCNN</li><li>seq2seq</li><li>attention</li><li>self attention</li><li>BERT</li><li>目标检测</li><li>推荐算法</li><li>生成网络</li><li>DQN</li><li>…</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;机器学习&quot;&gt;&lt;a href=&quot;#机器学习&quot; class=&quot;headerlink&quot; title=&quot;机器学习&quot;&gt;&lt;/a&gt;机器学习&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/2018/09/19/EasyML-1/&quot;&gt;机器学习简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a h
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://hf136.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://hf136.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>人工神经网络（Artificial Neural Network）</title>
    <link href="http://hf136.github.io/2018/11/18/neural-network/"/>
    <id>http://hf136.github.io/2018/11/18/neural-network/</id>
    <published>2018-11-18T02:30:33.000Z</published>
    <updated>2018-12-04T16:14:16.946Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>一个三层的神经网络模型如下：</p><p><img src="/resource/images/nn.png" alt="nn model"></p><a id="more"></a><p>神经网络可分为输入层、隐藏层和输出层，输入层一般除了输入的特征数据之外，还会有一个偏差项（bias）。神经网络一般会包含一个或者多个隐藏层，隐藏层一般由多个神经元（Neural Unit）构成，上图中的有一个隐藏层，隐藏层中有4个神经元。输出层根据具体不同的任务可以由神经元或者普通的线性回归等构成。</p><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经元一般由一个线性回归和一个激活函数（非线性部分）构成，例如：逻辑回归可以作为神经网络中的一种神经元。</p><p>常见的激活函数有：sigmoid函数、Relu、tanh等。</p><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><p>设输入层、隐藏层和输出层的单元个数分别为 n，l， k，则3层的神经网络一共有 $n \cdot l + l \cdot k$ 个参数。</p><p>$$<br>\begin{aligned}<br>z_2 =&amp; \Theta_1 X \\<br>a_2 =&amp; g(z_2) \\<br>z_3 =&amp; \Theta_2 a_2 \\<br>a_3 =&amp; g(z_3) \\<br>\hat{y} =&amp; a_3 \\<br>\end{aligned}<br>$$</p><p>其中，X 是输入的特征向量，$\hat{y}$ 是神经网络输出的结果，$\Theta_1$ 是一个 $n \cdot l$ 的参数矩阵（输入层为n，隐藏层为l），$\Theta_2$ 是隐藏层到输出层的参数矩阵，大小为 $l \cdot k$，$g(z)$ 为激活函数，这里使用sigmoid函数作为激活函数。这里的 $a, z$ 都是向量，函数 $g(z)$ 也是指对向量中的每一个元素做非线性变换。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>神经网络一般使用交叉熵，即使用和逻辑回归类似的损失函数，输出层的每一个输出单元是一个逻辑回归损失，并且求和。</p><p>$$<br>\begin{aligned}<br>loss =  J(\theta) = &amp; \frac{1}{m} \sum_{i=1}^{m} \sum_{i=1}^{K} Cost(\hat{y}^i_k - y^i_k) \\<br>= &amp; -  \frac{1}{m} \sum_{i=1}^{m} \sum_{i=1}^{K} [y^i_k \log{\hat{y}^i_k} + (1- y^i_k) \log{(1 - \hat{y}^i_k)}]<br>\end{aligned}<br>$$</p><p>其中，K 输出层输出单元个数， m 为训练样本数，$\hat{y}^i_k$ 为第i个样本的第k个输出单元的输出结果。</p><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>求使得损失函数最小的参数 ${\Theta}$ 。</p><p>$$\min_{\Theta} J(\Theta)$$</p><h2 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h2><p>使用链式求导法则计算梯度，$\Theta_2$ 的梯度为：</p><p>$$<br>\frac{\partial J}{\partial\Theta_2} = \frac{\partial J}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z_3}   \frac{\partial z_3}{\partial \Theta_2} = (-\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}) (\hat{y}(1-\hat{y})) \cdot a_2 = (\hat{y} - y) \cdot a_2<br>$$</p><p>$\Theta_1$ 的梯度为：</p><p>$$<br>\frac{\partial J}{\partial\Theta_1} = \frac{\partial J}{\partial \hat{y}}   \frac{\partial \hat{y}}{\partial z_3}   \frac{\partial z_3}{\partial a_2}    \frac{\partial a_2}{\partial z_2}  \frac{\partial z_2}{\partial \Theta_1} = {(\hat{y} - y) \Theta_2 [a_2(1-a_2)]} \cdot X<br>$$</p><p>这实际上也就是反向传播（Backpropagation）算法。令 $\delta^L = \hat{y} - y$ ，则 $\delta^{l-1} = \delta^l \Theta_{l-1} [a_{l-1}(1-a_{l-1})]$ ，梯度 $\Delta^l = \delta^{l+1} a_l$ 。</p><p><strong>注意</strong>：上面两个式子中的 $\Theta， a，z， \hat{y}$ 指的是矩阵或者向量中的元素分别求偏导数，上面这么写是为了简便。求导过程写成矩阵运算的形式也更方便，需注意进行相应的转置变换。</p><p>随着模型结构越来越复杂，每次都去计算梯度比较复杂，现在已经有很多深度学习框架可以进行<strong>自动微分</strong>计算梯度。下面我们还是使用链式求导法则来自己计算一下神经网络的梯度。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><a href="https://github.com/hf136/models/blob/master/ArtificialNeuralNetwork/raw_neural_network.py" target="_blank" rel="noopener">完整代码</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">input_size = <span class="number">2</span></span><br><span class="line">hidden_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成参数theta</span></span><br><span class="line">theta1 = np.random.rand(input_size + <span class="number">1</span>, hidden_size)</span><br><span class="line">theta2 = np.random.rand(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加偏差项</span></span><br><span class="line">ones = np.ones((X.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">X = np.concatenate((X, ones), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-1</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10001</span>):</span><br><span class="line">    <span class="comment"># 定义模型，前向计算（这里的隐藏层没有添加偏差项，也可以在每层隐藏层都加上偏差项）</span></span><br><span class="line">    z2 = X.dot(theta1)</span><br><span class="line">    a2 = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z2))</span><br><span class="line">    z3 = a2.dot(theta2)</span><br><span class="line">    pred_y = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z3))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss</span></span><br><span class="line">    loss = <span class="number">0.5</span> * np.square(pred_y - y).sum() / y.size</span><br><span class="line">    print(<span class="string">'epoch &#123;&#125;, loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出训练时的准确率</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        pred_label = pred_y &gt;= <span class="number">0.5</span></span><br><span class="line">        true_label = y &gt;= <span class="number">0.5</span></span><br><span class="line">        diff = pred_label == true_label</span><br><span class="line">        accuracy = diff.mean()</span><br><span class="line">        print(<span class="string">'accuracy &#123;&#125;'</span>.format(accuracy))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用链式求导计算梯度（和反向传播是一致的）</span></span><br><span class="line">    grad_z3 = pred_y - y</span><br><span class="line">    grad_theta2 = grad_z3.T.dot(a2).T</span><br><span class="line">    grad_a2 = grad_z3.dot(theta2.T)</span><br><span class="line">    grad_z2 = grad_a2 * a2 * (<span class="number">1</span> - a2)</span><br><span class="line">    grad_theta1 = X.T.dot(grad_z2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    theta2 -= learning_rate * grad_theta2</span><br><span class="line">    theta1 -= learning_rate * grad_theta1</span><br></pre></td></tr></table></figure><p>预测结果：</p><p><img src="https://github.com/hf136/models/raw/master/docs/images/ann_res.png" alt="lr res"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;模型&quot;&gt;&lt;a href=&quot;#模型&quot; class=&quot;headerlink&quot; title=&quot;模型&quot;&gt;&lt;/a&gt;模型&lt;/h2&gt;&lt;p&gt;一个三层的神经网络模型如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/images/nn.png&quot; alt=&quot;nn model&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://hf136.github.io/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://hf136.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>正则化（regularization）</title>
    <link href="http://hf136.github.io/2018/11/17/regularization/"/>
    <id>http://hf136.github.io/2018/11/17/regularization/</id>
    <published>2018-11-17T14:30:33.000Z</published>
    <updated>2018-11-18T03:18:29.636Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是正则化？"><a href="#什么是正则化？" class="headerlink" title="什么是正则化？"></a>什么是正则化？</h2><p>正则化主要的作用是防止模型过拟合，其原理是对网络中的参数进行惩罚（约束），防止网络模型中的参数过大而过于偏向某一个特征。常见的正则化有L1和L2正则化。</p><h2 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h2><p>对模型进行正则化一般是将正则项直接加到损失函数后面，L1正则化是把网络中所有的参数的绝对值相加。</p><p>$$loss_{regularization} = loss + \lambda \sum_{j=1}^{n} |\theta_j|$$</p><p>其中 $\lambda$ 为正则化系数，$n$ 为参数个数。</p><h2 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h2><p>$$loss_{regularization} = loss + \frac{\lambda}{2} \sum_{j=1}^{n} \theta_j^2$$</p><p>其中 $\lambda$ 为正则化系数（这里除于2是为了求导时计算简便），$n$ 为参数个数。</p>]]></content>
    
    <summary type="html">
    
       
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://hf136.github.io/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://hf136.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归（Logistic Regression）</title>
    <link href="http://hf136.github.io/2018/11/03/logistic-regression/"/>
    <id>http://hf136.github.io/2018/11/03/logistic-regression/</id>
    <published>2018-11-03T13:06:26.000Z</published>
    <updated>2018-11-17T13:11:37.184Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>逻辑回归实际上是在线性回归的基础上在加上一个sigmoid函数（非线性变换）：</p><p>$$<br>h_\theta(x) = \frac{1}{1 + e^{-\bf{\theta{^T}x}}}<br>$$</p><p>可分开写成：</p><p>$$<br>\begin{aligned}<br>h_\theta(x) =&amp; g({\bf{\theta{^T} x}}) \\<br>g(z) =&amp; \frac{1}{1 + e^{-z}}<br>\end{aligned}<br>$$</p><p>其中 $g(z)$ 称为sigmoid函数，函数图像为：</p><p><img src="/resource/images/sigmoid.png" alt="sigmoid image"></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>$$<br>\begin{aligned}<br>loss =  J(\theta) = &amp; \frac{1}{m} \sum_{i=1}^{m}Cost(h_\theta(x^i) - y^i) \\<br>= &amp; -  \frac{1}{m} [\sum_{i=1}^{m} y^i \log{h_\theta(x^i)} + (1- y^i) \log{(1 - h_\theta(x^i))}]<br>\end{aligned}<br>$$</p><p>逻辑回归输出的值在 0-1 之间，使用log损失，当标签 $y^i=1$ 时，预测结果 $h_\theta(x^i)$ 也为 1 时损失为0，预测结果与 1 相差越多，损失越大。当标签 $y^i=0$ 时同理。</p><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>求使得损失函数最小的参数 $\bf{\theta}$ 。</p><p>$$\min_{\theta} J(\theta)$$</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>求损失函数关于每一个参数 $\theta_j$ 的梯度并不断迭代。</p><p>$$\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m}{(h_\theta(x^i) - y^i)x_j^i}$$</p><p>其中，$x^i$ 表示第i条数据，$x_j$ 表示第j个特征，对应的参数为 $\theta_j$； 这个式子和线性回归的完全一致！</p><h2 id="具体求导过程"><a href="#具体求导过程" class="headerlink" title="具体求导过程"></a>具体求导过程</h2><p>sigmoid函数求导：<br>$$<br>\begin{aligned}<br>g^\prime(z) =&amp; \frac{d}{dz} \frac{1}{1 + e^{-z}} \\<br>=&amp; \frac{1}{(1 + e^{-z})^2} e^{-z} \\<br>=&amp; \frac{1}{1 + e^{-z}} (1 - \frac{1}{1 + e^{-z}}) \\<br>=&amp; g(z)(1-g(z)) \\<br>\end{aligned}<br>$$</p><p>对损失函数求 $\theta$ 的导数：<br>$$<br>\begin{aligned}<br>J^\prime(\theta_j) =&amp; -\frac{1}{m} \sum_{i=1}^{m} \frac{\partial}{\partial\theta_j} [y^i \log{h_\theta(x^i)} + (1- y^i) \log{(1 - h_\theta(x^i))}] \\<br>=&amp; -\frac{1}{m} \sum_{i=1}^{m} [y \frac{1}{h_\theta(x^i)} - (1-y)\frac{1}{1-h_\theta(x^i)}] \frac{\partial}{\partial\theta_j} g({\bf{\theta{^T} x}}) \\<br>=&amp; -\frac{1}{m} \sum_{i=1}^{m} [y \frac{1}{g({\bf{\theta{^T} x}})} - (1-y)\frac{1}{1-g({\bf{\theta{^T} x}})}] g({\bf{\theta{^T} x}})(1-g({\bf{\theta{^T} x}})) \frac{\partial}{\partial\theta_j} {\bf{\theta{^T} x}} \\<br>=&amp; -\frac{1}{m} \sum_{i=1}^{m} (y(1-g({\bf{\theta{^T} x}})) - (1-y)g({\bf{\theta{^T} x}})) x_j \\<br>=&amp; \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^i) - y) x_j<br>\end{aligned}<br>$$</p><p>对于更复杂的模型直接求导比较困难，目前比较流行的深度学习框架一般使用链式求导法则自动求导（应该说是自动微分）。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><a href="https://github.com/hf136/models/tree/master/LogisticRegression" target="_blank" rel="noopener">完整代码</a>放在了GitHub上，下面是核心的代码片段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义参数 w 和 b</span></span><br><span class="line">theta = np.random.rand(<span class="number">2</span>)</span><br><span class="line">bias = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 定义模型，前向计算</span></span><br><span class="line">    z = X.dot(theta) + bias</span><br><span class="line">    pred_y = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss</span></span><br><span class="line">    loss = - (y * np.log(pred_y) + (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - pred_y)).sum() / y.size</span><br><span class="line">    print(<span class="string">'epoch &#123;&#125;, loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算梯度（求导）</span></span><br><span class="line">    grad_theta = (pred_y - y).T.dot(X) / y.size</span><br><span class="line">    grad_bias = (pred_y - y).sum() / y.size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    theta -= learning_rate * grad_theta</span><br><span class="line">    bias -= learning_rate * grad_bias</span><br></pre></td></tr></table></figure><p>预测结果：</p><p><img src="https://github.com/hf136/models/raw/master/docs/images/logis-reg.png" alt="lr res"></p>]]></content>
    
    <summary type="html">
    
       
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://hf136.github.io/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://hf136.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>线性回归（Linear Regression）</title>
    <link href="http://hf136.github.io/2018/09/27/linear-regression/"/>
    <id>http://hf136.github.io/2018/09/27/linear-regression/</id>
    <published>2018-09-27T14:30:33.000Z</published>
    <updated>2018-12-04T15:39:24.701Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>$$y={\bf{wx}} +b$$</p><p>其中，${\bf{w}}$ 和 ${\bf{x}}$ 都是向量， ${\bf{w}} = w_1, w_2, …, w_n$ 表示要学习的模型参数, ${\bf{x}} = x_1, x_2, …, x_n$ 表示模型的输入。</p><h2 id="损失函数（代价函数）"><a href="#损失函数（代价函数）" class="headerlink" title="损失函数（代价函数）"></a>损失函数（代价函数）</h2><p>$$L({\bf{w}}, b) = \frac{1}{2m} \sum_{i=1}^{m}{(y^\prime_i - y_i)^2}$$</p><p>其中，$m$ 表示训练样本数， $y^\prime$ 表示模型输出结果， $y$ 表示实际结果。</p><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>求使得损失函数最小的参数 ${\bf{w}}$ 和 $b$ 。</p><p>$$\min_{w,b}L({\bf{w}}, b)$$</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>对损失函数求每个 $w_j$ 和 $b$ 的偏导数，并通过下式不断迭代得到较优的参数：</p><p>$$w_j := w_j - \alpha \frac{1}{m} \sum_{i=1}^{m}{(y^\prime_i - y_i)x_j}$$</p><p>$$b := b - \alpha \frac{1}{m} \sum_{i=1}^{m}{(y^\prime_i - y_i)}$$</p><p>其中， $\alpha$ 为学习速率， $\alpha$ 后面的项为偏导数。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><a href="https://github.com/hf136/models/tree/master/LinearRegression" target="_blank" rel="noopener">完整代码</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义参数 w 和 b</span></span><br><span class="line">w = random.random()</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 定义模型，前向计算</span></span><br><span class="line">    pred_y = w * x + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss</span></span><br><span class="line">    loss = <span class="number">0.5</span> * np.square(pred_y - y).sum() / y.size</span><br><span class="line">    print(<span class="string">'epoch &#123;&#125;, loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算梯度（求导）</span></span><br><span class="line">    grad_w = ((pred_y - y) * x).sum() / y.size</span><br><span class="line">    grad_b = (pred_y - y).sum() / y.size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    w -= learning_rate * grad_w</span><br><span class="line">    b -= learning_rate * grad_b</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
       
    
    </summary>
    
      <category term="Machine Learning" scheme="http://hf136.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://hf136.github.io/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://hf136.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>函数的导数</title>
    <link href="http://hf136.github.io/2018/09/20/math/"/>
    <id>http://hf136.github.io/2018/09/20/math/</id>
    <published>2018-09-20T13:09:46.000Z</published>
    <updated>2018-09-27T14:11:53.592Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常见基本函数导数"><a href="#常见基本函数导数" class="headerlink" title="常见基本函数导数"></a>常见基本函数导数</h2><table><thead><tr><th>导数名</th><th>原函数</th><th>导函数</th></tr></thead><tbody><tr><td>常函数（常数）</td><td>$y=C$ (C为常数)</td><td>$y^\prime=0$</td></tr><tr><td>幂函数</td><td>$y=x^n$</td><td>$y^\prime=nx^{n-1}$</td></tr><tr><td>指数函数</td><td>$y=a^x$</td><td>$y^\prime=a^x\ln x$</td></tr><tr><td></td><td>$y=e^x$</td><td>$y^\prime=e^x$</td></tr><tr><td>对数函数</td><td>$y=\log_a x$</td><td>$y^\prime=\frac{1}{x\ln a}$</td></tr><tr><td></td><td>$y=\ln x$</td><td>$y^\prime=\frac{1}{x}$</td></tr><tr><td>正弦函数</td><td>$y=\sin x$</td><td>$y^\prime=\cos x$</td></tr><tr><td>余弦函数</td><td>$y=\cos x$</td><td>$y^\prime=-\sin x$</td></tr></tbody></table><h2 id="复合函数求导"><a href="#复合函数求导" class="headerlink" title="复合函数求导"></a>复合函数求导</h2><p>原函数：$y^\prime=f(g(x))$， 其中 $y=f(u)$， $u=g(x)$</p><p>使用链式法则求导：$y^\prime = f^\prime(u)u^\prime(x) = f^\prime(g(x))g^\prime(x)$</p><h2 id="导数的四则运算"><a href="#导数的四则运算" class="headerlink" title="导数的四则运算"></a>导数的四则运算</h2><p>$$(u \pm v)^\prime = u^\prime \pm v^\prime$$</p><p>$$(uv)^\prime = u^\prime v + u v^\prime$$</p><p>$$(\frac{u}{v})^\prime = \frac{u^\prime v - u v^\prime}{v^2}$$</p>]]></content>
    
    <summary type="html">
    
      整理一些常见的函数的导数
    
    </summary>
    
      <category term="Math" scheme="http://hf136.github.io/categories/Math/"/>
    
    
      <category term="Math" scheme="http://hf136.github.io/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门——什么是机器学习？</title>
    <link href="http://hf136.github.io/2018/09/19/EasyML-1/"/>
    <id>http://hf136.github.io/2018/09/19/EasyML-1/</id>
    <published>2018-09-19T15:09:46.000Z</published>
    <updated>2018-11-18T02:48:14.360Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h2><p>有人说，现在的人工智能就是 $ y=ax+b $ ，虽然可能只是开玩笑，但其实我觉得这句话很有道理。</p><p>我们先来看一个经典的房价预测问题。为了将问题变得更加简单，我们只考虑一种因素：房子面积。下表给出了一组（构造的）数据，房子面积（平方米）和对应的房子价格（万元）。</p><p><img src="/resource/images/lr-1.png" alt="data"></p><p>我们目的是构建一个模型，根据房子面积来预测出房子价格。只要你输入房子的面积，系统就能预测出房子的价格。我们从图中可以看出，房子的价格和房子面积具有线性关系，我们可以人为的画出一条直线，如下图所示，使用这条直线来作为预测房价的模型。而这条直线可以使用 $ y=ax+b $ 来表示，只要我们知道了a和b的值，通过输入房子面积$ x $，就能预测出房价价格 $ y $。</p><blockquote><p>对于这个问题来说，所谓的机器学习就是：通过已有的房价数据，机器可以自动计算（学习）得到a和b，这样这个问题就解决了。</p></blockquote><p>在机器学习没有出现之前，我们是可以通过一些人为方式来计算出a和b的。比如我先画出下图的直线，然后使用量角器量出直线与x轴的夹角，就能得到斜率从而计算出a，然后在直线随便找一个点(x1, y1)带入到式子中，就可以计算出b；或者直接带入两个点（x1, y1）、（x2, y2），通过解方程的方式计算出 a 和 b。</p><p><img src="/resource/images/lr-2.png" alt="line"></p><h2 id="如何自动学习得到a和b？"><a href="#如何自动学习得到a和b？" class="headerlink" title="如何自动学习得到a和b？"></a>如何自动学习得到a和b？</h2><p>一般的做法是：我们可以先随机得到一个a和b的值，比如a=1，b=0，然后在不断的去调整a和b的值，最终得到最优的 a 和 b。<br>为了能够使计算机能够自动学习得到 a 和 b，我们需要一个衡量指标，那就是损失函数（loss function）或者称作代价函数（cost function），它的作用的衡量模型的好坏。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在这个问题中，我们可以使用平均平方误差来当做我们的损失函数，即：我们有 m 条训练数据，我们可以通过公式 $ loss = \frac {1} {m} \sum_{i=0}^m (y_i - y_i^\prime)^2 $ 来计算模型预测的值 $y^\prime$ 和真实值 $y$ 之间的平方误差。</p><p>当模型预测的准确率是100%的时候，损失函数的值等于0，也就是说损失函数的结果越小，模型的效果越好。有了这个损失函数，我们就可以知道哪些 a 和 b 的值是比较好的，这样机器就可以知道哪些 a 和 b 的值是比较好的了。</p><p>我们很容易想到：我们可以将所有可能的 a 和 b 的值遍历一遍，通过已有的数据去计算损失函数的值，取损失函数最小时所对应的 a 和 b 的值，那问题不就解决了吗？</p><p>是的，但是这种方法太笨了，a 和 b 的值是有无限多可能的，而且计算量太大，所以我们还需要一种有效的学习方法：<strong>梯度下降</strong>。</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>什么是梯度下降（Gradient Descent）呢？</p><p>在这个问题中，我们的损失函数实际上是一个二次函数，我们可以简单的理解为 a 或 b 和对应损失函数的值之间的关系如下图所示：</p><p><img src="/resource/images/lr-loss.png" alt="loss"></p><p>在上图中，横坐标表示 a 的取值，纵坐标表示 loss 的值。我们可以知道当 a=2.3 时, 损失函数 loss 的值最小，那如果一开始我们随机得到的 a 的初始值是1.7或者3.6的话，怎么样才能快速的得到一个接近最优值2.3的值呢？</p><blockquote><p>答案是：导数（只有一个变量是导数，在多变量中为偏导数）。</p><p>我们可以发现：当 a &gt; 2.3 时，比如说 a = 3.6，此时 a 的导数（切线的斜率）大于0；当 a &lt; 2.3 时，比如说 a = 1.7，此时 a 的导数小于0；而当 a = 2.3 时，a 的导数等于0。</p></blockquote><p>假设一开我们随机得到: a = 3.6，那么 a 需要向左移动，即计算 a 的导数 $ \nabla_a    $，就可以知道 a 需要调整移动的方向。我们可以通过以下公式得到新的 a 值 $a_{new}$ 为：$$ a_{new}=a_{old} - \alpha\nabla_a $$</p><p>其中 $\alpha$ 是一个大于 0 的系数，通常称作<strong>学习率</strong>，控制着 a 每一次调整的步长。如下图所示，a 每次的调整如果过大的话，很容易调整过头；如果每次的调整过小的话，学习的速度就会非常慢。所以我们需要一个学习率去调整控制学习的速度。<br><br><img src="/resource/images/lr-loss2.png" alt="loss"></p><p>如上图所示，经过多次迭代，我们就可以快速得到一个无限接近于最优值的 $ a_{best}$ ，同理，b 的最优值 $b_{best}$ 也可以通过这种方式得到。$$ b_{new}=b_{old} - \alpha\nabla_b $$</p><p>梯度下降中，<strong>梯度</strong>指的是 a 和 b 的导数（实际上是偏导数）组成的向量：$(\nabla_a, \nabla_b)$ ；<strong>下降</strong>是指损失函数值 loss 不断减小的过程。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这里，机器学习就是:</p><blockquote><p>在已有的数据（训练数据）的基础上，先建立一个数学模型 $y=ax+b$，再定义一个损失函数 $loss$，最后通过梯度下降的方式不断的调整模型参数 a 和 b，使损失函数的值不断变小，得到最优的参数 $ a_{best}$ 和 $ b_{best}$ 的过程。</p></blockquote><p>推广到一般的情况（考虑多种影响房价的因素），这里的 $x$ 变成了一组值 $x_1, x_2, … , x_n$, a也相应的变成了一组值，为了更加形象一些，一般我们使用 weight（权重）的首字母 $w$ 来代替 a ，即：$w_1, w_2, … , w_n$，这里的线性回归模型就变成了：$$y=w_1 x_1 + w_2 x_2 + … w_n x_n + b$$</p><p>同样，我们可以通过计算每一个 $w_i$ 的偏导数，使用梯度下降的学习方法来不断迭代更新得到最优的 $w$ 值。我们可以将上面的公式简写成向量内积的形式： $ y=\vec{w} \cdot \vec{x} + b $</p><h2 id="再谈谈深度学习"><a href="#再谈谈深度学习" class="headerlink" title="再谈谈深度学习"></a>再谈谈深度学习</h2><p>深度学习可谓是人工智能领域中最“智能”的分支，而深度学习中无论多么复杂的模型都离不开 $ y=\vec{w} \cdot \vec{x} + b $ ，因为它是“神经元”的重要组成部分。</p><p>神经元一般由 $ y=\vec{w} \cdot \vec{x} + b $ （线性部分）加上一个激活函数（非线性部分）组成。</p><p>在深度学习中，无论多么复杂的模型几乎都离不开<strong>神经元</strong>，神经元以不同的“空间结构”组合在一起，构成了各种各样的复杂模型。</p>]]></content>
    
    <summary type="html">
    
      有人说，现在的人工智能就是 $ y=ax+b $ ，虽然可能是开玩笑，但其实我觉得这句话很有道理。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://hf136.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://hf136.github.io/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://hf136.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
