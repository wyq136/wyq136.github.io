<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Attention——深度学习中的注意力机制</title>
      <link href="/2019/01/27/attention/"/>
      <url>/2019/01/27/attention/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是-Attention-？"><a href="#什么是-Attention-？" class="headerlink" title="什么是 Attention ？"></a>什么是 Attention ？</h2><p>我们来一起看着下面这张图片，并且读一下下面这句话。</p><blockquote><p>一只黄色的小猫带着一个鹿角帽子趴在沙发上。</p></blockquote><p><img src="/resource/images/cat.png" alt="cat"></p><p>在读这句话的过程中，你的注意力是不是会发生变化？我相信大多数人是这样的：当读到“小猫”的时候，注意力在猫身上；当读到“鹿角帽子”的时候，注意力在鹿角帽子上。</p><p>这就是人类的注意力，它是会随着时间发生变化的。</p><a id="more"></a><h2 id="神经网络中的-Attention-机制"><a href="#神经网络中的-Attention-机制" class="headerlink" title="神经网络中的 Attention 机制"></a>神经网络中的 Attention 机制</h2><h3 id="seq2seq-模型"><a href="#seq2seq-模型" class="headerlink" title="seq2seq 模型"></a>seq2seq 模型</h3><p><img src="/resource/images/seq2seq.png" alt="seq2seq"></p><p>seq2seq 是指 sequence to sequence，这类模型的输入是一个序列 x1、x2、x3 …，输出也是一个序列 y1、y2 …。它通常由两个类似于 LSTM 的循环神经网络（RNN）构成，也常被称作 Encoder-Decoder 模型，第一个 RNN 进行 encode，第二个 RNN 作为 decode。</p><p>一般的 Encoder-Decoder 模型，Encoder阶段可以表示为：</p><p>$$<br>\begin{aligned}<br>    h_t =&amp; f(x_t, h_{t-1}) \\<br>    c   =&amp; q(h_1, … ,h_T) \\<br>\end{aligned}<br>$$</p><p>其中： $h_t$ 是 n 维实数向量，表示编码阶段RNN的 t 时刻的隐藏状态；c 是各个时刻隐藏状态生成的向量；𝑓 和 𝑞 是非线性函数。例如：$f$ 可以是 LSTM，$q$ 函数取最后一个时刻的输出结果，$q({h_1, … ,h_{T_x}}) = h_T$ 。</p><p>Decoder 阶段可以表示为：</p><p>$$<br>\begin{aligned}<br>    y_t =&amp; g(y_{t-1}, s_t, c) \\<br>\end{aligned}<br>$$</p><p>其中， $y_t$ 是 t 时刻的输出结果，初始化 $y_0$ 为零向量，$s_t$ 为 t 时刻隐藏层状态，c 为 Encoder 阶段的输出结果；g 是非线性函数。例如：g 是一个 LSTM 单元，把 $y_{t-1}， c$ 拼起来当做输入。</p><p>在机器翻译、对话生成等场景中经常会用到这类模型，但这类模型是有一些局限性的。</p><ul><li>局限性：编码和解码之间的唯一联系就是一个固定长度的语义向量 C ；在输入序列较长的情况下信息损失更加严重。</li></ul><h3 id="Attention-模型原理"><a href="#Attention-模型原理" class="headerlink" title="Attention 模型原理"></a>Attention 模型原理</h3><p>由于单纯的seq2seq会存在一些问题，于是人类便发明在神经网络加入模拟人类注意力的机制。</p><p>下面我们为上面的 Encoder-Decoder 模型加入一种 Attention 机制。</p><p>Attention 机制改变的是 Encoder-Decoder 模型中的 Decoder 阶段；Encoder 阶段不变，将 Decoder 阶段变成：</p><p>$$<br>\begin{aligned}<br>    y_t =&amp; g(y_{t-1}, s_t, c_t)<br>\end{aligned}<br>$$</p><p>单一向量 c 变成了一组向量 $c_t$ ，它的计算方式为：</p><p>$$<br>c_t = \sum_j^T{\alpha_{tj} h_j}<br>$$</p><p>其中， $h_j \in {(h_1, … ,h_T)}$ ，即 $h_j$ 为Encoder阶段j时刻的输出，$\alpha_{tj}$ 为t时刻第j个隐藏层的权重系数，它可以通过一个前馈神经网络学习得到：</p><p>$$<br>\begin{aligned}<br>    c_{tj} = \frac{\rm{exp}(e_{tj})} {\sum_{k=1}^T{ \rm{exp}(e_{tk}) }} \\<br>    e_{tj} = a(s_{t-1}, h_j)<br>\end{aligned}<br>$$</p><p>其中，𝑎 是一个前馈神经网络，比如：𝑎 可以是一个由 tanh 作为激活函数的神经元。$s_{t-1}$ 为t-1时刻的decoder阶段的隐藏层状态，$h_j$ encoder 阶段 j 时刻的输出。</p><p>总的来说就是：</p><blockquote><p>t 时刻的向量 $c_t$ 是由encoder阶段各个时刻的输出 $h_t$ 加权得到的结果；<br>而这个加权的权重是由decoder阶段t-1时刻的隐藏层状态 $s_{t-1}$ 和encoder阶段各个时刻的输出 $h_t$ 通过一个前馈神经网络并归一化之后的结果。</p></blockquote><h3 id="Attention-机制可视化"><a href="#Attention-机制可视化" class="headerlink" title="Attention 机制可视化"></a>Attention 机制可视化</h3><p><img src="/resource/images/attention-1.png" alt="attention model"></p><p>图中，横轴为：输入的英文单词序列，纵轴为：输出的法语单词序列；每一行是权值 𝛼 组成的向量；越亮的地方权重越大。</p><p>这相当于实现了一种 “软对齐” 机制，所以注意力机制有时也叫做 “对齐模型” （Alignment Model）。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">https://arxiv.org/abs/1409.0473</a><br><a href="https://mp.weixin.qq.com/s/_Ru6GMcrSO25bTs8vM6FmA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/_Ru6GMcrSO25bTs8vM6FmA</a></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>LSTM</title>
      <link href="/2019/01/19/LSTM/"/>
      <url>/2019/01/19/LSTM/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>LSTM全称是 Long Short Term Memory Network（长短时记忆网络），它也是一种循环神经网络（RNN）算法。</p><a id="more"></a><p>在普通的 RNN 中，经常会出现以下两个问题：</p><ul><li>梯度爆炸：梯度太大导致程序出错</li><li>梯度消失：原始RNN无法处理长距离依赖</li></ul><p>梯度爆炸相对来说比较好解决，比如：可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。而相对于梯度消失来说，这个会比较难解决一些。</p><p>LSTM 算法的出现就是为了解决梯度消失的问题。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>为了解决 RNN 模型中长距离依赖梯度消失的问题，LSTM 中引入了一个新的记忆单元，以及三个门：输入门、遗忘门和输出门。</p><p>所有门使用的激活函数都是sigmoid函数，也就是说三个门的向量中的元素的值都在（0, 1）之间，三个门的作用可以理解为：</p><blockquote><p>输入门：控制当前时刻的输入，决定需要吸收当前多少比例的输入<br>遗忘门：控制长期记忆的遗忘比例，决定有多少“记忆”被保留，有多少被遗忘<br>输出门：控制当前时刻的输出，决定当前时刻的结果需要输出多少</p></blockquote><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><p><img src="/resource/images/lstm-cell.png" alt="lstm cell"></p><p>$$<br>\begin{aligned}<br>i_t =&amp; \sigma(W_{ii}x_t + W_{hi}h_{t-1} + b_i) \\<br>f_t =&amp; \sigma(W_{if}x_t + W_{hf}h_{t-1} + b_f) \\<br>o_t =&amp; \sigma(W_{io}x_t + W_{ho}h_{t-1} + b_o) \\<br>g_t =&amp; \tanh(W_{ig}x_t + W_{hg}h_{t-1} + b_g) \\<br>c_t =&amp; f_t \times c_{t-1} + i_t \times g_t \\<br>h_t =&amp; o_t \times \tanh c_t \\<br>\end{aligned}<br>$$</p><p>在上图和公式中，$i_t, f_t, o_t$ 分别为输入、遗忘、输出门； $g_t$ （图中为$c^\prime_i$）是当前时刻前馈计算的结果； $c_t$ 是长期记忆单元， $h_t$ 是这一个时刻 LSTM 网络的输出结果。$\sigma$ 是 sigmoid 激活函数，$\times$ 是指向量的对应值相乘。</p><h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>LSTM 的梯度计算和 RNN 的差不多，只是多了一些参数复杂一些。我们可以使用梯度检查的方法来检验计算的梯度是否正确。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><a href="https://github.com/hf136/models/tree/master/LSTM" target="_blank" rel="noopener">完整代码</a></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>循环神经网络（Recurrent Neural Network）</title>
      <link href="/2018/12/28/rnn/"/>
      <url>/2018/12/28/rnn/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在自然语言处理（NLP）中，需要处理的数据通常都是不定长的。例如，我们要构建一个神经网络模型，将下面这两句话翻译成英文：</p><ul><li>这一世诺言从不曾忘。</li><li>深度学习的概念源于人工神经网络的研究。</li></ul><p>这两句话的长度是不一样的，一般的神经网络输入的特征纬度是固定的，显然不能很好的解决这个问题，于是便出现了循环神经网络（Recurrent Neural Network，RNN）。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="基本的循环神经网络"><a href="#基本的循环神经网络" class="headerlink" title="基本的循环神经网络"></a>基本的循环神经网络</h3><p>一个最基本的循环神经网络由输入层，隐藏层和输出层构成，如下图所示：</p><p><img src="/resource/images/rnn-1.jpg" alt="rnn-1"></p><a id="more"></a><p>这里的x、s、o分别是输入层、隐藏层和输出层，它们都是一个向量；W、U、V是连接层与层之间的权重矩阵。<br>RNN和一般的神经网络最大不同在于：</p><blockquote><p>RNN多了一个从隐藏层到隐藏层($s =&gt; s$)的过程，使RNN拥有了“记忆”的功能。<br>(注意：这里的s要把他它看层多个隐藏层的多个神经单元，s是隐藏层单元构成的向量)</p></blockquote><p>在RNN网络中，我们需要引入一个时间（顺序）的概念，我们把上图展开，RNN可以画成这样：</p><p><img src="/resource/images/rnn-2.jpg" alt="rnn-2"></p><p>从图中可以看到，t 时刻的RNN网络输入值是 $x_t$，输出值是 $o_t$，隐藏层的值是 $s_t$，它的值取决于输入值 $x_t$ 和 t-1 时刻的隐藏层的值 $s_{t-1}$。</p><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><p>RNN的每时间步的计算过程如下：<br>$$<br>\begin{aligned}<br>s_t =&amp; g(Ux_t + Ws_{t-1} + b) \\<br>o_t =&amp; f(Vs_t)<br>\end{aligned}<br>$$</p><p>其中，g、f 是激活函数，s 为隐藏层的值， b 是偏差项；隐藏层 s 的初始值 $s_0$ 为零向量。</p><p>可以看出，RNN网络最后输出的结果受到所有输入序列 $x_1, x_2 … x_T$ 的影响。因为隐藏 $s_{t-1}$ 保存了前面 t-1 个 x 值的结果，隐藏层 s 充当了一个“记忆”的角色。</p><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>同样是求使得损失函数最小的权重 U 、 W 、V 、b；损失函数的形式根据具体的任务会有所不同。</p><h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>这里RNN使用到的计算梯度的算法是BPTT（Back Propagation Trough Time），加上了时间的概念，是一种基于时间的反向传播算法。</p><p>虽然名字听上去很高大上的样子，但其实并不复杂，和普通的反向传播算法也差不多，把RNN展开之后，一样可以使用链式求导，这其实就很简单了。</p><p><img src="/resource/images/rnn-3.png" alt="RNN backward"></p><p>如上图所示，把 RNN 展开之后，RNN 每一时刻的反向传播求导过程和普通的神经网络是一样。根据任务的不同，有可能每一个时刻都有误差传递，也可能只有最后一个时刻有误差传递。</p><p>展开后的 RNN 可以看成共享权重的全连接神经网络模型，只要使用链式求导分别求出每一个时刻的权重梯度，最后再把所有时刻的梯度相加求和就可以得到最终的 RNN 权重梯度。</p><h2 id="梯度爆炸和梯度消失"><a href="#梯度爆炸和梯度消失" class="headerlink" title="梯度爆炸和梯度消失"></a>梯度爆炸和梯度消失</h2><p>在序列很长的时候，RNN 模型训练过程中，很容易出现梯度爆炸（梯度很大）或者梯度消失（梯度几乎为0）的问题，导致模型无法正常拟合。</p><p>这是为什么呢？</p><p>链式求导求解梯度的过程其实一个连乘的过程：<br>$$<br>\frac{\partial{S_n}}{\partial{S_{n-1}}} \frac{\partial{S_{n-1}}}{\partial{S_{n-2}}} … \frac{\partial{S_2}}{\partial{S_1}}<br>$$<br>当序列很长的时候，如果每个阶段梯度都大于1的话，梯度就会爆炸，比如: $10^9$ ；如果每个阶段梯度都小于1的话，梯度就会消失，比如: $0.1^9$ ；</p><p>对于梯度消失，其实指的是长距离的梯度消失，即长距离依赖会消失，训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。也就是说 RNN 的“记忆力”有限，在处理较长的序列时，往往会“忘记”序列前面的内容。由于整个模型的梯度是各个时刻梯度之和，所以整个模型的梯度还不会消失。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>这里实现了一个简单的 RNN 模型，其中激活函数使用的是 Relu 激活函数。<a href="https://github.com/hf136/models/tree/master/RNN" target="_blank" rel="noopener">完整代码</a></p><p>一个 RNN 时间步的计算过程，其实就和普通的神经网络是一致的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNCell</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    一个 RNN 时间步的计算过程</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_size, hidden_size)</span>:</span></span><br><span class="line">        self.in_size = in_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.w_i2h = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, (in_size, hidden_size))</span><br><span class="line">        self.w_h2h = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, (hidden_size, hidden_size))</span><br><span class="line">        self.bias = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, (<span class="number">1</span>, hidden_size))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x[x &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, h)</span>:</span></span><br><span class="line">        self.i2h = x.dot(self.w_i2h)</span><br><span class="line">        self.h2h = h.dot(self.w_h2h)</span><br><span class="line">        self.h_relu = self.relu(self.i2h + self.h2h + self.bias)</span><br><span class="line">        <span class="keyword">return</span> self.h_relu</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad, i, h)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> i.ndim == <span class="number">1</span>:</span><br><span class="line">            i = np.expand_dims(i, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> h.ndim == <span class="number">1</span>:</span><br><span class="line">            h = np.expand_dims(h, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self.grad_h_relu = grad</span><br><span class="line">        self.grad_h = self.grad_h_relu.copy()</span><br><span class="line">        self.grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        self.grad_w_h2h = h.T.dot(self.grad_h)</span><br><span class="line">        self.grad_w_i2h = i.T.dot(self.grad_h)</span><br><span class="line">        self.grad_bias = self.grad_h</span><br><span class="line">        self.grad_h_in = self.grad_h.dot(self.w_h2h.T)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.grad_h_in</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weight</span><span class="params">(self, lr)</span>:</span></span><br><span class="line">        self.w_i2h -= lr * self.grad_w_i2h</span><br><span class="line">        self.w_h2h -= lr * self.grad_w_h2h</span><br><span class="line">        self.bias -= lr * self.grad_bias</span><br></pre></td></tr></table></figure><p>完整的 RNN 序列计算过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    完整的 RNN 序列计算过程</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_size, hidden_size)</span>:</span></span><br><span class="line">        self.h_state = []</span><br><span class="line">        self.in_size = in_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.rnncell = RNNCell(in_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.h_state = []</span><br><span class="line">        self.x = x</span><br><span class="line">        h = np.zeros(self.hidden_size)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">            self.h_state.append(h)</span><br><span class="line">            h = self.rnncell.forward(i, h)</span><br><span class="line">        self.h_out = h</span><br><span class="line">        <span class="keyword">return</span> self.h_out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad)</span>:</span></span><br><span class="line">        self.grad_w_i2h = np.zeros((self.in_size, self.hidden_size))</span><br><span class="line">        self.grad_w_h2h = np.zeros((self.hidden_size, self.hidden_size))</span><br><span class="line">        self.grad_bias = np.zeros((<span class="number">1</span>, self.hidden_size))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.h_state) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            x = self.x[i]</span><br><span class="line">            h = self.h_state[i]</span><br><span class="line">            grad = self.rnncell.backward(grad, x, h)</span><br><span class="line">            self.grad_w_i2h += self.rnncell.grad_w_i2h</span><br><span class="line">            self.grad_w_h2h += self.rnncell.grad_w_h2h</span><br><span class="line">            self.grad_bias += self.rnncell.grad_bias</span><br><span class="line">        <span class="keyword">return</span> grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weight</span><span class="params">(self, lr)</span>:</span></span><br><span class="line">        self.rnncell.w_i2h -= lr * self.grad_w_i2h</span><br><span class="line">        self.rnncell.w_h2h -= lr * self.grad_w_h2h</span><br><span class="line">        self.rnncell.bias -= lr * self.grad_bias</span><br><span class="line">        <span class="keyword">return</span> self.rnncell.w_i2h, self.rnncell.w_h2h, self.rnncell.bias</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">https://zybuluo.com/hanbingtao/note/541458</a></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>人工神经网络（Artificial Neural Network）</title>
      <link href="/2018/11/18/neural-network/"/>
      <url>/2018/11/18/neural-network/</url>
      
        <content type="html"><![CDATA[<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>一个三层的神经网络模型如下：</p><p><img src="/resource/images/nn.png" alt="nn model"></p><a id="more"></a><p>神经网络可分为输入层、隐藏层和输出层，输入层一般除了输入的特征数据之外，还会有一个偏差项（bias）。神经网络一般会包含一个或者多个隐藏层，隐藏层一般由多个神经元（Neural Unit）构成，上图中的有一个隐藏层，隐藏层中有4个神经元。输出层根据具体不同的任务可以由神经元或者普通的线性回归等构成。</p><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经元一般由一个线性回归和一个激活函数（非线性部分）构成，例如：逻辑回归可以作为神经网络中的一种神经元。</p><p>常见的激活函数有：sigmoid函数、Relu、tanh等。</p><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><p>设输入层、隐藏层和输出层的单元个数分别为 n，l， k，则3层的神经网络一共有 $n \cdot l + l \cdot k$ 个参数。</p><p>$$<br>\begin{aligned}<br>z_2 =&amp; \Theta_1 X \\<br>a_2 =&amp; g(z_2) \\<br>z_3 =&amp; \Theta_2 a_2 \\<br>a_3 =&amp; g(z_3) \\<br>\hat{y} =&amp; a_3 \\<br>\end{aligned}<br>$$</p><p>其中，X 是输入的特征向量，$\hat{y}$ 是神经网络输出的结果，$\Theta_1$ 是一个 $n \cdot l$ 的参数矩阵（输入层为n，隐藏层为l），$\Theta_2$ 是隐藏层到输出层的参数矩阵，大小为 $l \cdot k$，$g(z)$ 为激活函数，这里使用sigmoid函数作为激活函数。这里的 $a, z$ 都是向量，函数 $g(z)$ 也是指对向量中的每一个元素做非线性变换。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>神经网络一般使用交叉熵，即使用和逻辑回归类似的损失函数，输出层的每一个输出单元是一个逻辑回归损失，并且求和。</p><p>$$<br>\begin{aligned}<br>loss =  J(\theta) = &amp; \frac{1}{m} \sum_{i=1}^{m} \sum_{i=1}^{K} Cost(\hat{y}^i_k - y^i_k) \\<br>= &amp; -  \frac{1}{m} \sum_{i=1}^{m} \sum_{i=1}^{K} [y^i_k \log{\hat{y}^i_k} + (1- y^i_k) \log{(1 - \hat{y}^i_k)}]<br>\end{aligned}<br>$$</p><p>其中，K 输出层输出单元个数， m 为训练样本数，$\hat{y}^i_k$ 为第i个样本的第k个输出单元的输出结果。</p><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>求使得损失函数最小的参数 ${\Theta}$ 。</p><p>$$\min_{\Theta} J(\Theta)$$</p><h2 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h2><p>使用链式求导法则计算梯度，$\Theta_2$ 的梯度为：</p><p>$$<br>\frac{\partial J}{\partial\Theta_2} = \frac{\partial J}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z_3}   \frac{\partial z_3}{\partial \Theta_2} = (-\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}) (\hat{y}(1-\hat{y})) \cdot a_2 = (\hat{y} - y) \cdot a_2<br>$$</p><p>$\Theta_1$ 的梯度为：</p><p>$$<br>\frac{\partial J}{\partial\Theta_1} = \frac{\partial J}{\partial \hat{y}}   \frac{\partial \hat{y}}{\partial z_3}   \frac{\partial z_3}{\partial a_2}    \frac{\partial a_2}{\partial z_2}  \frac{\partial z_2}{\partial \Theta_1} = {(\hat{y} - y) \Theta_2 [a_2(1-a_2)]} \cdot X<br>$$</p><p>这实际上也就是反向传播（Backpropagation）算法。令 $\delta^L = \hat{y} - y$ ，则 $\delta^{l-1} = \delta^l \Theta_{l-1} [a_{l-1}(1-a_{l-1})]$ ，梯度 $\Delta^l = \delta^{l+1} a_l$ 。</p><p><strong>注意</strong>：上面两个式子中的 $\Theta， a，z， \hat{y}$ 指的是矩阵或者向量中的元素分别求偏导数，上面这么写是为了简便。求导过程写成矩阵运算的形式也更方便，需注意进行相应的转置变换。</p><p>随着模型结构越来越复杂，每次都去计算梯度比较复杂，现在已经有很多深度学习框架可以进行<strong>自动微分</strong>计算梯度。下面我们还是使用链式求导法则来自己计算一下神经网络的梯度。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><a href="https://github.com/hf136/models/blob/master/ArtificialNeuralNetwork/raw_neural_network.py" target="_blank" rel="noopener">完整代码</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">input_size = <span class="number">2</span></span><br><span class="line">hidden_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成参数theta</span></span><br><span class="line">theta1 = np.random.rand(input_size + <span class="number">1</span>, hidden_size)</span><br><span class="line">theta2 = np.random.rand(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加偏差项</span></span><br><span class="line">ones = np.ones((X.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">X = np.concatenate((X, ones), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-1</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10001</span>):</span><br><span class="line">    <span class="comment"># 定义模型，前向计算（这里的隐藏层没有添加偏差项，也可以在每层隐藏层都加上偏差项）</span></span><br><span class="line">    z2 = X.dot(theta1)</span><br><span class="line">    a2 = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z2))</span><br><span class="line">    z3 = a2.dot(theta2)</span><br><span class="line">    pred_y = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z3))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss</span></span><br><span class="line">    loss = - (y * np.log(pred_y) + (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - pred_y)).mean()</span><br><span class="line">    print(<span class="string">'epoch &#123;&#125;, loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出训练时的准确率</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        pred_label = pred_y &gt;= <span class="number">0.5</span></span><br><span class="line">        true_label = y &gt;= <span class="number">0.5</span></span><br><span class="line">        diff = pred_label == true_label</span><br><span class="line">        accuracy = diff.mean()</span><br><span class="line">        print(<span class="string">'accuracy &#123;&#125;'</span>.format(accuracy))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用链式求导计算梯度（和反向传播是一致的）</span></span><br><span class="line">    grad_z3 = pred_y - y</span><br><span class="line">    grad_theta2 = a2.T.dot(grad_z3)</span><br><span class="line">    grad_a2 = grad_z3.dot(theta2.T)</span><br><span class="line">    grad_z2 = grad_a2 * a2 * (<span class="number">1</span> - a2)</span><br><span class="line">    grad_theta1 = X.T.dot(grad_z2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    theta2 -= learning_rate * grad_theta2</span><br><span class="line">    theta1 -= learning_rate * grad_theta1</span><br></pre></td></tr></table></figure><p>预测结果：</p><p><img src="https://github.com/hf136/models/raw/master/docs/images/ann_res.png" alt="lr res"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>正则化（regularization）</title>
      <link href="/2018/11/17/regularization/"/>
      <url>/2018/11/17/regularization/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是正则化？"><a href="#什么是正则化？" class="headerlink" title="什么是正则化？"></a>什么是正则化？</h2><p>正则化主要的作用是防止模型过拟合，其原理是对网络中的参数进行惩罚（约束），防止网络模型中的参数过大而过于偏向某一个特征。常见的正则化有L1和L2正则化。</p><h2 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h2><p>对模型进行正则化一般是将正则项直接加到损失函数后面，L1正则化是把网络中所有的参数的绝对值相加。</p><p>$$loss_{regularization} = loss + \lambda \sum_{j=1}^{n} |\theta_j|$$</p><p>其中 $\lambda$ 为正则化系数，$n$ 为参数个数。</p><h2 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h2><p>$$loss_{regularization} = loss + \frac{\lambda}{2} \sum_{j=1}^{n} \theta_j^2$$</p><p>其中 $\lambda$ 为正则化系数（这里除于2是为了求导时计算简便），$n$ 为参数个数。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>逻辑回归（Logistic Regression）</title>
      <link href="/2018/11/03/logistic-regression/"/>
      <url>/2018/11/03/logistic-regression/</url>
      
        <content type="html"><![CDATA[<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>逻辑回归实际上是在线性回归的基础上在加上一个sigmoid函数（非线性变换）：</p><p>$$<br>h_\theta(x) = \frac{1}{1 + e^{-\bf{\theta{^T}x}}}<br>$$</p><p>可分开写成：</p><p>$$<br>\begin{aligned}<br>h_\theta(x) =&amp; g({\bf{\theta{^T} x}}) \\<br>g(z) =&amp; \frac{1}{1 + e^{-z}}<br>\end{aligned}<br>$$</p><p>其中 $g(z)$ 称为sigmoid函数，函数图像为：</p><p><img src="/resource/images/sigmoid.png" alt="sigmoid image"></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>$$<br>\begin{aligned}<br>loss =  J(\theta) = &amp; \frac{1}{m} \sum_{i=1}^{m}Cost(h_\theta(x^i) - y^i) \\<br>= &amp; -  \frac{1}{m} [\sum_{i=1}^{m} y^i \log{h_\theta(x^i)} + (1- y^i) \log{(1 - h_\theta(x^i))}]<br>\end{aligned}<br>$$</p><p>逻辑回归输出的值在 0-1 之间，使用log损失，当标签 $y^i=1$ 时，预测结果 $h_\theta(x^i)$ 也为 1 时损失为0，预测结果与 1 相差越多，损失越大。当标签 $y^i=0$ 时同理。</p><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>求使得损失函数最小的参数 $\bf{\theta}$ 。</p><p>$$\min_{\theta} J(\theta)$$</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>求损失函数关于每一个参数 $\theta_j$ 的梯度并不断迭代。</p><p>$$\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m}{(h_\theta(x^i) - y^i)x_j^i}$$</p><p>其中，$x^i$ 表示第i条数据，$x_j$ 表示第j个特征，对应的参数为 $\theta_j$； 这个式子和线性回归的完全一致！</p><h2 id="具体求导过程"><a href="#具体求导过程" class="headerlink" title="具体求导过程"></a>具体求导过程</h2><p>sigmoid函数求导：<br>$$<br>\begin{aligned}<br>g^\prime(z) =&amp; \frac{d}{dz} \frac{1}{1 + e^{-z}} \\<br>=&amp; \frac{1}{(1 + e^{-z})^2} e^{-z} \\<br>=&amp; \frac{1}{1 + e^{-z}} (1 - \frac{1}{1 + e^{-z}}) \\<br>=&amp; g(z)(1-g(z)) \\<br>\end{aligned}<br>$$</p><p>对损失函数求 $\theta$ 的导数：<br>$$<br>\begin{aligned}<br>J^\prime(\theta_j) =&amp; -\frac{1}{m} \sum_{i=1}^{m} \frac{\partial}{\partial\theta_j} [y^i \log{h_\theta(x^i)} + (1- y^i) \log{(1 - h_\theta(x^i))}] \\<br>=&amp; -\frac{1}{m} \sum_{i=1}^{m} [y \frac{1}{h_\theta(x^i)} - (1-y)\frac{1}{1-h_\theta(x^i)}] \frac{\partial}{\partial\theta_j} g({\bf{\theta{^T} x}}) \\<br>=&amp; -\frac{1}{m} \sum_{i=1}^{m} [y \frac{1}{g({\bf{\theta{^T} x}})} - (1-y)\frac{1}{1-g({\bf{\theta{^T} x}})}] g({\bf{\theta{^T} x}})(1-g({\bf{\theta{^T} x}})) \frac{\partial}{\partial\theta_j} {\bf{\theta{^T} x}} \\<br>=&amp; -\frac{1}{m} \sum_{i=1}^{m} (y(1-g({\bf{\theta{^T} x}})) - (1-y)g({\bf{\theta{^T} x}})) x_j \\<br>=&amp; \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^i) - y) x_j<br>\end{aligned}<br>$$</p><p>对于更复杂的模型直接求导比较困难，目前比较流行的深度学习框架一般使用链式求导法则自动求导（应该说是自动微分）。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><a href="https://github.com/hf136/models/tree/master/LogisticRegression" target="_blank" rel="noopener">完整代码</a>放在了GitHub上，下面是核心的代码片段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义参数 w 和 b</span></span><br><span class="line">theta = np.random.rand(<span class="number">2</span>)</span><br><span class="line">bias = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 定义模型，前向计算</span></span><br><span class="line">    z = X.dot(theta) + bias</span><br><span class="line">    pred_y = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss</span></span><br><span class="line">    loss = - (y * np.log(pred_y) + (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - pred_y)).mean()</span><br><span class="line">    print(<span class="string">'epoch &#123;&#125;, loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算梯度（求导）</span></span><br><span class="line">    grad_theta = (pred_y - y).T.dot(X) / y.size</span><br><span class="line">    grad_bias = (pred_y - y).sum() / y.size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    theta -= learning_rate * grad_theta</span><br><span class="line">    bias -= learning_rate * grad_bias</span><br></pre></td></tr></table></figure><p>预测结果：</p><p><img src="https://github.com/hf136/models/raw/master/docs/images/logis-reg.png" alt="lr res"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>线性回归（Linear Regression）</title>
      <link href="/2018/09/27/linear-regression/"/>
      <url>/2018/09/27/linear-regression/</url>
      
        <content type="html"><![CDATA[<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>$$y={\bf{wx}} +b$$</p><p>其中，${\bf{w}}$ 和 ${\bf{x}}$ 都是向量， ${\bf{w}} = w_1, w_2, …, w_n$ 表示要学习的模型参数, ${\bf{x}} = x_1, x_2, …, x_n$ 表示模型的输入。</p><h2 id="损失函数（代价函数）"><a href="#损失函数（代价函数）" class="headerlink" title="损失函数（代价函数）"></a>损失函数（代价函数）</h2><p>$$L({\bf{w}}, b) = \frac{1}{2m} \sum_{i=1}^{m}{(y^\prime_i - y_i)^2}$$</p><p>其中，$m$ 表示训练样本数， $y^\prime$ 表示模型输出结果， $y$ 表示实际结果。</p><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>求使得损失函数最小的参数 ${\bf{w}}$ 和 $b$ 。</p><p>$$\min_{w,b}L({\bf{w}}, b)$$</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>对损失函数求每个 $w_j$ 和 $b$ 的偏导数，并通过下式不断迭代得到较优的参数：</p><p>$$w_j := w_j - \alpha \frac{1}{m} \sum_{i=1}^{m}{(y^\prime_i - y_i)x_j}$$</p><p>$$b := b - \alpha \frac{1}{m} \sum_{i=1}^{m}{(y^\prime_i - y_i)}$$</p><p>其中， $\alpha$ 为学习速率， $\alpha$ 后面的项为偏导数。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><a href="https://github.com/hf136/models/tree/master/LinearRegression" target="_blank" rel="noopener">完整代码</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义参数 w 和 b</span></span><br><span class="line">w = random.random()</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 定义模型，前向计算</span></span><br><span class="line">    pred_y = w * x + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss</span></span><br><span class="line">    loss = <span class="number">0.5</span> * np.square(pred_y - y).mean()</span><br><span class="line">    print(<span class="string">'epoch &#123;&#125;, loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算梯度（求导）</span></span><br><span class="line">    grad_w = ((pred_y - y) * x).mean()</span><br><span class="line">    grad_b = (pred_y - y).mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    w -= learning_rate * grad_w</span><br><span class="line">    b -= learning_rate * grad_b</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>函数的导数</title>
      <link href="/2018/09/20/math/"/>
      <url>/2018/09/20/math/</url>
      
        <content type="html"><![CDATA[<h2 id="常见基本函数导数"><a href="#常见基本函数导数" class="headerlink" title="常见基本函数导数"></a>常见基本函数导数</h2><table><thead><tr><th>导数名</th><th>原函数</th><th>导函数</th></tr></thead><tbody><tr><td>常函数（常数）</td><td>$y=C$ (C为常数)</td><td>$y^\prime=0$</td></tr><tr><td>幂函数</td><td>$y=x^n$</td><td>$y^\prime=nx^{n-1}$</td></tr><tr><td>指数函数</td><td>$y=a^x$</td><td>$y^\prime=a^x\ln x$</td></tr><tr><td></td><td>$y=e^x$</td><td>$y^\prime=e^x$</td></tr><tr><td>对数函数</td><td>$y=\log_a x$</td><td>$y^\prime=\frac{1}{x\ln a}$</td></tr><tr><td></td><td>$y=\ln x$</td><td>$y^\prime=\frac{1}{x}$</td></tr><tr><td>正弦函数</td><td>$y=\sin x$</td><td>$y^\prime=\cos x$</td></tr><tr><td>余弦函数</td><td>$y=\cos x$</td><td>$y^\prime=-\sin x$</td></tr></tbody></table><h2 id="复合函数求导"><a href="#复合函数求导" class="headerlink" title="复合函数求导"></a>复合函数求导</h2><p>原函数：$y^\prime=f(g(x))$， 其中 $y=f(u)$， $u=g(x)$</p><p>使用链式法则求导：$y^\prime = f^\prime(u)u^\prime(x) = f^\prime(g(x))g^\prime(x)$</p><h2 id="导数的四则运算"><a href="#导数的四则运算" class="headerlink" title="导数的四则运算"></a>导数的四则运算</h2><p>$$(u \pm v)^\prime = u^\prime \pm v^\prime$$</p><p>$$(uv)^\prime = u^\prime v + u v^\prime$$</p><p>$$(\frac{u}{v})^\prime = \frac{u^\prime v - u v^\prime}{v^2}$$</p>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Math </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习入门——什么是机器学习？</title>
      <link href="/2018/09/19/EasyML-1/"/>
      <url>/2018/09/19/EasyML-1/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h2><p>有人说，现在的人工智能就是 $ y=ax+b $ ，虽然可能只是开玩笑，但其实我觉得这句话很有道理。</p><p>我们先来看一个经典的房价预测问题。为了将问题变得更加简单，我们只考虑一种因素：房子面积。下表给出了一组（构造的）数据，房子面积（平方米）和对应的房子价格（万元）。</p><p><img src="/resource/images/lr-1.png" alt="data"></p><p>我们目的是构建一个模型，根据房子面积来预测出房子价格。只要你输入房子的面积，系统就能预测出房子的价格。我们从图中可以看出，房子的价格和房子面积具有线性关系，我们可以人为的画出一条直线，如下图所示，使用这条直线来作为预测房价的模型。而这条直线可以使用 $ y=ax+b $ 来表示，只要我们知道了a和b的值，通过输入房子面积$ x $，就能预测出房价价格 $ y $。</p><blockquote><p>对于这个问题来说，所谓的机器学习就是：通过已有的房价数据，机器可以自动计算（学习）得到a和b，这样这个问题就解决了。</p></blockquote><p>在机器学习没有出现之前，我们是可以通过一些人为方式来计算出a和b的。比如我先画出下图的直线，然后使用量角器量出直线与x轴的夹角，就能得到斜率从而计算出a，然后在直线随便找一个点(x1, y1)带入到式子中，就可以计算出b；或者直接带入两个点（x1, y1）、（x2, y2），通过解方程的方式计算出 a 和 b。</p><p><img src="/resource/images/lr-2.png" alt="line"></p><h2 id="如何自动学习得到a和b？"><a href="#如何自动学习得到a和b？" class="headerlink" title="如何自动学习得到a和b？"></a>如何自动学习得到a和b？</h2><p>一般的做法是：我们可以先随机得到一个a和b的值，比如a=1，b=0，然后在不断的去调整a和b的值，最终得到最优的 a 和 b。<br>为了能够使计算机能够自动学习得到 a 和 b，我们需要一个衡量指标，那就是损失函数（loss function）或者称作代价函数（cost function），它的作用的衡量模型的好坏。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在这个问题中，我们可以使用平均平方误差来当做我们的损失函数，即：我们有 m 条训练数据，我们可以通过公式 $ loss = \frac {1} {m} \sum_{i=0}^m (y_i - y_i^\prime)^2 $ 来计算模型预测的值 $y^\prime$ 和真实值 $y$ 之间的平方误差。</p><p>当模型预测的准确率是100%的时候，损失函数的值等于0，也就是说损失函数的结果越小，模型的效果越好。有了这个损失函数，我们就可以知道哪些 a 和 b 的值是比较好的，这样机器就可以知道哪些 a 和 b 的值是比较好的了。</p><p>我们很容易想到：我们可以将所有可能的 a 和 b 的值遍历一遍，通过已有的数据去计算损失函数的值，取损失函数最小时所对应的 a 和 b 的值，那问题不就解决了吗？</p><p>是的，但是这种方法太笨了，a 和 b 的值是有无限多可能的，而且计算量太大，所以我们还需要一种有效的学习方法：<strong>梯度下降</strong>。</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>什么是梯度下降（Gradient Descent）呢？</p><p>在这个问题中，我们的损失函数实际上是一个二次函数，我们可以简单的理解为 a 或 b 和对应损失函数的值之间的关系如下图所示：</p><p><img src="/resource/images/lr-loss.png" alt="loss"></p><p>在上图中，横坐标表示 a 的取值，纵坐标表示 loss 的值。我们可以知道当 a=2.3 时, 损失函数 loss 的值最小，那如果一开始我们随机得到的 a 的初始值是1.7或者3.6的话，怎么样才能快速的得到一个接近最优值2.3的值呢？</p><blockquote><p>答案是：导数（只有一个变量是导数，在多变量中为偏导数）。</p><p>我们可以发现：当 a &gt; 2.3 时，比如说 a = 3.6，此时 a 的导数（切线的斜率）大于0；当 a &lt; 2.3 时，比如说 a = 1.7，此时 a 的导数小于0；而当 a = 2.3 时，a 的导数等于0。</p></blockquote><p>假设一开我们随机得到: a = 3.6，那么 a 需要向左移动，即计算 a 的导数 $ \nabla_a    $，就可以知道 a 需要调整移动的方向。我们可以通过以下公式得到新的 a 值 $a_{new}$ 为：$$ a_{new}=a_{old} - \alpha\nabla_a $$</p><p>其中 $\alpha$ 是一个大于 0 的系数，通常称作<strong>学习率</strong>，控制着 a 每一次调整的步长。如下图所示，a 每次的调整如果过大的话，很容易调整过头；如果每次的调整过小的话，学习的速度就会非常慢。所以我们需要一个学习率去调整控制学习的速度。<br><br><img src="/resource/images/lr-loss2.png" alt="loss"></p><p>如上图所示，经过多次迭代，我们就可以快速得到一个无限接近于最优值的 $ a_{best}$ ，同理，b 的最优值 $b_{best}$ 也可以通过这种方式得到。$$ b_{new}=b_{old} - \alpha\nabla_b $$</p><p>梯度下降中，<strong>梯度</strong>指的是 a 和 b 的导数（实际上是偏导数）组成的向量：$(\nabla_a, \nabla_b)$ ；<strong>下降</strong>是指损失函数值 loss 不断减小的过程。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这里，机器学习就是:</p><blockquote><p>在已有的数据（训练数据）的基础上，先建立一个数学模型 $y=ax+b$，再定义一个损失函数 $loss$，最后通过梯度下降的方式不断的调整模型参数 a 和 b，使损失函数的值不断变小，得到最优的参数 $ a_{best}$ 和 $ b_{best}$ 的过程。</p></blockquote><p>推广到一般的情况（考虑多种影响房价的因素），这里的 $x$ 变成了一组值 $x_1, x_2, … , x_n$, a也相应的变成了一组值，为了更加形象一些，一般我们使用 weight（权重）的首字母 $w$ 来代替 a ，即：$w_1, w_2, … , w_n$，这里的线性回归模型就变成了：$$y=w_1 x_1 + w_2 x_2 + … w_n x_n + b$$</p><p>同样，我们可以通过计算每一个 $w_i$ 的偏导数，使用梯度下降的学习方法来不断迭代更新得到最优的 $w$ 值。我们可以将上面的公式简写成向量内积的形式： $ y=\vec{w} \cdot \vec{x} + b $</p><h2 id="再谈谈深度学习"><a href="#再谈谈深度学习" class="headerlink" title="再谈谈深度学习"></a>再谈谈深度学习</h2><p>深度学习可谓是人工智能领域中最“智能”的分支，而深度学习中无论多么复杂的模型都离不开 $ y=\vec{w} \cdot \vec{x} + b $ ，因为它是“神经元”的重要组成部分。</p><p>神经元一般由 $ y=\vec{w} \cdot \vec{x} + b $ （线性部分）加上一个激活函数（非线性部分）组成。</p><p>在深度学习中，无论多么复杂的模型几乎都离不开<strong>神经元</strong>，神经元以不同的“空间结构”组合在一起，构成了各种各样的复杂模型。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
